{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL6MfXjHr1oA"
      },
      "source": [
        "# **Model Training Process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UC0kjXsCV5"
      },
      "source": [
        "**1) Using Ultralytics as shortcut of whole pytorch modelling procedure.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPkcbx-HtPe7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac4ac90-bca4-4a2d-b195-7802ed1b5192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 41.7/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Install YOLOv8 (Ultralytics)\n",
        "!pip install ultralytics\n",
        "\n",
        "# Check if installed\n",
        "import ultralytics\n",
        "ultralytics.checks()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzJbeBVStST0"
      },
      "source": [
        "**2) Mouting Google drive as data source\n",
        "(Data downloaded from kaggle ; Link : https://www.kaggle.com/datasets/ziya07/multi-class-fabric-defect-detection-dataset)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OO2c_SX4rGtT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b3b3e3-3235-41bf-eb61-38e86c2e2316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3ut58CdtWys"
      },
      "source": [
        "**3) Creating a file called data.yaml in Colab notebook**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whLtMbz5rKpX"
      },
      "outputs": [],
      "source": [
        "yaml_content = \"\"\"\n",
        "train: /content/drive/MyDrive/Colab_Notebooks/Dataset/images/train\n",
        "val: /content/drive/MyDrive/Colab_Notebooks/Dataset/images/val\n",
        "test: /content/drive/MyDrive/Colab_Notebooks/Dataset/images/test\n",
        "\n",
        "nc: 6\n",
        "names: [ 'Broken stitch', 'Hole', 'Lines', 'Needle mark', 'Pinched fabric', 'Stain' ]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fabric_data.yaml\", \"w\") as f:\n",
        "    f.write(yaml_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFUi5ih3sJP3"
      },
      "source": [
        "Remapping class numbers as i have deleted two classes (Horizontals & Vertical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bzq-kXqtrNd7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Class index remapping\n",
        "class_map = {\n",
        "    0: 0,  # Broken stitch\n",
        "    1: 1,  # Hole\n",
        "    3: 2,  # Lines\n",
        "    4: 3,  # Needle mark\n",
        "    5: 4,  # Pinched fabric\n",
        "    6: 5,  # Stain\n",
        "}\n",
        "\n",
        "def remap_classes(label_folder):\n",
        "    for fname in os.listdir(label_folder):\n",
        "        if not fname.endswith(\".txt\"):\n",
        "            continue\n",
        "        path = os.path.join(label_folder, fname)\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        updated = []\n",
        "        for line in lines:\n",
        "            if not line.strip(): continue\n",
        "            parts = line.strip().split()\n",
        "            cls = int(parts[0])\n",
        "            if cls in class_map:\n",
        "                parts[0] = str(class_map[cls])\n",
        "                updated.append(\" \".join(parts))\n",
        "            else:\n",
        "                # Skip removed classes\n",
        "                continue\n",
        "\n",
        "        with open(path, 'w') as f:\n",
        "            f.write(\"\\n\".join(updated) + \"\\n\")\n",
        "\n",
        "# Run on all 3 sets\n",
        "remap_classes('/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/train')\n",
        "remap_classes('/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/val')\n",
        "remap_classes('/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIAPoC_ztHwa"
      },
      "source": [
        "**4) Data Preprocessing\n",
        "[Annotations (labels of images created using Cvat.ai) and organising raw data from kaggle into test,train and Validation already done and uploaded in drive. Here only processes line identifying and cleaning is done]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umkf1ecgrZWQ",
        "outputId": "c79836d7-178f-4b3e-8dc1-9a5d911c243d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corrupt or invalid label files (89):\n",
            " ['Needle mark_A_08_014.txt', 'lines_line_2018-10-10 11_17_16.895198.txt', 'Needle mark_A_08_095.txt', 'Needle mark_A_08_108.txt', 'Needle mark_A_08_087.txt', 'Needle mark_A_08_046.txt', 'Needle mark_A_08_023.txt', 'hole_10.txt', 'Pinched fabric_A_03_020.txt', 'Needle mark_A_08_102.txt', 'Needle mark_A_08_043.txt', 'Needle mark_A_08_027.txt', 'Needle mark_A_08_022.txt', 'Needle mark_A_08_020.txt', 'Needle mark_A_08_066.txt', 'Needle mark_A_08_021.txt', 'Needle mark_A_08_100.txt', 'Needle mark_A_08_032.txt', 'Needle mark_A_08_090.txt', 'Needle mark_A_08_016.txt', 'Needle mark_A_08_069.txt', 'Needle mark_A_08_026.txt', 'Needle mark_A_08_019.txt', 'Needle mark_A_08_034.txt', 'Needle mark_A_08_106.txt', 'Needle mark_A_08_029.txt', 'Needle mark_A_08_105.txt', 'Needle mark_A_08_079.txt', 'Needle mark_A_08_091.txt', 'Needle mark_A_08_037.txt', 'Needle mark_A_08_093.txt', 'Pinched fabric_A_03_026.txt', 'Needle mark_A_08_082.txt', 'Needle mark_A_08_015.txt', 'Needle mark_A_08_097.txt', 'Pinched fabric_A_03_025.txt', 'Needle mark_A_08_061.txt', 'Needle mark_A_08_075.txt', 'Needle mark_A_08_089.txt', 'Needle mark_A_08_049.txt', 'Needle mark_A_08_054.txt', 'Needle mark_A_08_048.txt', 'Needle mark_A_08_039.txt', 'Needle mark_A_08_074.txt', 'Needle mark_A_08_025.txt', 'Needle mark_A_08_083.txt', 'Needle mark_A_08_002.txt', 'Needle mark_A_08_071.txt', 'Pinched fabric_A_03_024.txt', 'Needle mark_A_08_008.txt', 'Pinched fabric_A_03_017.txt', 'Needle mark_A_08_041.txt', 'Needle mark_A_08_030.txt', 'Needle mark_A_08_003.txt', 'Needle mark_A_08_101.txt', 'Needle mark_A_08_088.txt', 'Needle mark_A_08_042.txt', 'Needle mark_A_08_107.txt', 'Needle mark_A_08_060.txt', 'Needle mark_A_08_086.txt', 'Needle mark_A_08_038.txt', 'Needle mark_A_08_080.txt', 'Needle mark_A_08_011.txt', 'Needle mark_A_08_077.txt', 'Needle mark_A_08_013.txt', 'Needle mark_A_08_064.txt', 'Needle mark_A_08_044.txt', 'Needle mark_A_08_031.txt', 'Needle mark_A_08_084.txt', 'Needle mark_A_08_035.txt', 'Pinched fabric_A_03_016.txt', 'Pinched fabric_A_03_019.txt', 'Needle mark_A_08_052.txt', 'Needle mark_A_08_047.txt', 'Needle mark_A_08_094.txt', 'Needle mark_A_08_098.txt', 'Needle mark_A_08_053.txt', 'Needle mark_A_08_018.txt', 'Pinched fabric_A_03_021.txt', 'Needle mark_A_08_103.txt', 'Needle mark_A_08_056.txt', 'Needle mark_A_08_070.txt', 'hole_1.txt', 'lines_line_2018-10-10 10_44_57.141603.txt', 'lines_line_2018-10-10 11_29_32.499997.txt', 'lines_line_2018-10-10 10_31_31.252520.txt', 'lines_line_2018-10-10 11_57_43.558575.txt', 'stain_337.txt', 'stain_378.txt']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "label_dir = \"/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/train\"\n",
        "bad_labels = []\n",
        "\n",
        "for fname in os.listdir(label_dir):\n",
        "    if fname.endswith(\".txt\"):\n",
        "        path = os.path.join(label_dir, fname)\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    bad_labels.append(fname)\n",
        "                    break\n",
        "                if not all(p.replace('.', '', 1).isdigit() or p.isdigit() for p in parts):\n",
        "                    bad_labels.append(fname)\n",
        "                    break\n",
        "                if int(parts[0]) >= 8:\n",
        "                    bad_labels.append(fname)\n",
        "                    break\n",
        "\n",
        "print(f\"Corrupt or invalid label files ({len(bad_labels)}):\\n\", bad_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9ba2TmBtfN4"
      },
      "source": [
        "Missing annotations does't affect model infact helps in training so it is not erased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIBaujbvrb5q",
        "outputId": "44b9db92-dfa9-4111-f84c-96ab9f31f546"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images with missing labels: 0\n",
            "Sample missing: []\n",
            "Empty label files (no annotations): 0\n",
            "Sample empty: []\n"
          ]
        }
      ],
      "source": [
        "image_dir = \"/content/drive/MyDrive/Colab_Notebooks/Dataset/images/train\"\n",
        "label_dir = \"/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/train\"\n",
        "\n",
        "import os\n",
        "\n",
        "image_files = [f[:-4] for f in os.listdir(image_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "label_files = [f[:-4] for f in os.listdir(label_dir) if f.endswith('.txt')]\n",
        "\n",
        "missing_labels = [img for img in image_files if img not in label_files]\n",
        "empty_labels = [f for f in label_files if os.path.getsize(os.path.join(label_dir, f + \".txt\")) == 0]\n",
        "\n",
        "print(f\"Images with missing labels: {len(missing_labels)}\")\n",
        "print(\"Sample missing:\", missing_labels[:5])\n",
        "print(f\"Empty label files (no annotations): {len(empty_labels)}\")\n",
        "print(\"Sample empty:\", empty_labels[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSI-QOkstqhf"
      },
      "source": [
        "**I have not added defect free images so i am adding that along with creating there labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpQViR70re-w",
        "outputId": "9ff39299-5f99-4e82-dc2a-8f0eb570e0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Successfully copied 500 defect-free images and created empty labels.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# ✅ SET YOUR PATHS (Update if needed)\n",
        "source_images_path = \"/content/drive/MyDrive/Colab_Notebooks/Dataset/images/defect free\"\n",
        "target_images_path = \"/content/drive/MyDrive/Colab_Notebooks/Dataset/images/train\"\n",
        "target_labels_path = \"/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/train\"\n",
        "\n",
        "# ✅ Check if source folder exists\n",
        "if not os.path.exists(source_images_path):\n",
        "    raise FileNotFoundError(f\"Source folder not found: {source_images_path}\")\n",
        "\n",
        "# ✅ Make sure target folders exist\n",
        "os.makedirs(target_images_path, exist_ok=True)\n",
        "os.makedirs(target_labels_path, exist_ok=True)\n",
        "\n",
        "# ✅ Step 1: Collect image filenames\n",
        "all_images = [f for f in os.listdir(source_images_path) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
        "\n",
        "# ✅ Step 2: Randomly select 500\n",
        "selected_images = random.sample(all_images, 500)\n",
        "\n",
        "# ✅ Step 3: Copy images + create empty label files\n",
        "for img_file in selected_images:\n",
        "    src_img = os.path.join(source_images_path, img_file)\n",
        "    dst_img = os.path.join(target_images_path, img_file)\n",
        "    shutil.copy(src_img, dst_img)\n",
        "\n",
        "    label_name = os.path.splitext(img_file)[0] + \".txt\"\n",
        "    dst_label = os.path.join(target_labels_path, label_name)\n",
        "    with open(dst_label, \"w\") as f:\n",
        "        pass  # creates an empty label file\n",
        "\n",
        "print(f\"✅ Successfully copied 500 defect-free images and created empty labels.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd2jGF0Gtwl9"
      },
      "source": [
        "Deleting Empty labels of train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X49Qzqy7riBl",
        "outputId": "c931c10e-a211-4882-c8ab-15dfd622c274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted orphan label: stain_369.txt\n",
            "Deleted orphan label: stain_278.txt\n",
            "Deleted orphan label: stain_388.txt\n",
            "Deleted orphan label: stain_7.txt\n",
            "Deleted orphan label: stain_52.txt\n",
            "Deleted orphan label: stain_215.txt\n",
            "Deleted orphan label: stain_63.txt\n",
            "Deleted orphan label: stain_373.txt\n",
            "Deleted orphan label: stain_364.txt\n",
            "Deleted orphan label: stain_249.txt\n",
            "Deleted orphan label: stain_107.txt\n",
            "Deleted orphan label: stain_114.txt\n",
            "Deleted orphan label: stain_102.txt\n",
            "Deleted orphan label: stain_111.txt\n",
            "Deleted orphan label: stain_109.txt\n",
            "Deleted orphan label: stain_10.txt\n",
            "Deleted orphan label: stain_108.txt\n",
            "Deleted orphan label: stain_101.txt\n",
            "Deleted orphan label: stain_105.txt\n",
            "Deleted orphan label: stain_135.txt\n",
            "Deleted orphan label: stain_124.txt\n",
            "Deleted orphan label: stain_130.txt\n",
            "Deleted orphan label: stain_139.txt\n",
            "Deleted orphan label: stain_132.txt\n",
            "Deleted orphan label: stain_131.txt\n",
            "Deleted orphan label: stain_123.txt\n",
            "Deleted orphan label: stain_138.txt\n",
            "Deleted orphan label: stain_121.txt\n",
            "Deleted orphan label: stain_126.txt\n",
            "Deleted orphan label: stain_133.txt\n",
            "Deleted orphan label: stain_171.txt\n",
            "Deleted orphan label: stain_166.txt\n",
            "Deleted orphan label: stain_227.txt\n",
            "Deleted orphan label: stain_167.txt\n",
            "Deleted orphan label: stain_196.txt\n",
            "Deleted orphan label: stain_213.txt\n",
            "Deleted orphan label: stain_145.txt\n",
            "Deleted orphan label: stain_175.txt\n",
            "Deleted orphan label: stain_203.txt\n",
            "Deleted orphan label: stain_143.txt\n",
            "Deleted orphan label: stain_214.txt\n",
            "Deleted orphan label: stain_210.txt\n",
            "Deleted orphan label: stain_232.txt\n",
            "Deleted orphan label: stain_174.txt\n",
            "Deleted orphan label: stain_142.txt\n",
            "Deleted orphan label: stain_182.txt\n",
            "Deleted orphan label: stain_183.txt\n",
            "Deleted orphan label: stain_209.txt\n",
            "Deleted orphan label: stain_228.txt\n",
            "Deleted orphan label: stain_140.txt\n",
            "Deleted orphan label: stain_164.txt\n",
            "Deleted orphan label: stain_235.txt\n",
            "Deleted orphan label: stain_254.txt\n",
            "Deleted orphan label: stain_257.txt\n",
            "Deleted orphan label: stain_237.txt\n",
            "Deleted orphan label: stain_236.txt\n",
            "Deleted orphan label: stain_273.txt\n",
            "Deleted orphan label: stain_309.txt\n",
            "Deleted orphan label: stain_253.txt\n",
            "Deleted orphan label: stain_282.txt\n",
            "Deleted orphan label: stain_258.txt\n",
            "Deleted orphan label: stain_265.txt\n",
            "Deleted orphan label: stain_276.txt\n",
            "Deleted orphan label: stain_298.txt\n",
            "Deleted orphan label: stain_297.txt\n",
            "Deleted orphan label: stain_256.txt\n",
            "Deleted orphan label: stain_272.txt\n",
            "Deleted orphan label: stain_252.txt\n",
            "Deleted orphan label: stain_259.txt\n",
            "Deleted orphan label: stain_243.txt\n",
            "Deleted orphan label: stain_242.txt\n",
            "Deleted orphan label: stain_241.txt\n",
            "Deleted orphan label: stain_277.txt\n",
            "Deleted orphan label: stain_244.txt\n",
            "Deleted orphan label: stain_267.txt\n",
            "Deleted orphan label: stain_275.txt\n",
            "Deleted orphan label: stain_295.txt\n",
            "Deleted orphan label: stain_234.txt\n",
            "Deleted orphan label: stain_317.txt\n",
            "Deleted orphan label: stain_271.txt\n",
            "Deleted orphan label: stain_269.txt\n",
            "Deleted orphan label: stain_268.txt\n",
            "Deleted orphan label: stain_238.txt\n",
            "Deleted orphan label: stain_279.txt\n",
            "Deleted orphan label: stain_280.txt\n",
            "Deleted orphan label: stain_248.txt\n",
            "Deleted orphan label: stain_246.txt\n",
            "Deleted orphan label: stain_260.txt\n",
            "Deleted orphan label: stain_266.txt\n",
            "Deleted orphan label: stain_322.txt\n",
            "Deleted orphan label: stain_326.txt\n",
            "Deleted orphan label: stain_321.txt\n",
            "Deleted orphan label: stain_320.txt\n",
            "Deleted orphan label: stain_327.txt\n",
            "Deleted orphan label: stain_319.txt\n",
            "Deleted orphan label: stain_324.txt\n",
            "Deleted orphan label: stain_346.txt\n",
            "Deleted orphan label: stain_384.txt\n",
            "Deleted orphan label: stain_74.txt\n",
            "Deleted orphan label: stain_362.txt\n",
            "Deleted orphan label: stain_66.txt\n",
            "Deleted orphan label: stain_44.txt\n",
            "Deleted orphan label: stain_391.txt\n",
            "Deleted orphan label: stain_351.txt\n",
            "Deleted orphan label: stain_354.txt\n",
            "Deleted orphan label: stain_341.txt\n",
            "Deleted orphan label: stain_355.txt\n",
            "Deleted orphan label: stain_53.txt\n",
            "Deleted orphan label: stain_72.txt\n",
            "Deleted orphan label: stain_65.txt\n",
            "Deleted orphan label: stain_42.txt\n",
            "Deleted orphan label: stain_365.txt\n",
            "Deleted orphan label: stain_389.txt\n",
            "Deleted orphan label: stain_376.txt\n",
            "Deleted orphan label: stain_395.txt\n",
            "Deleted orphan label: stain_359.txt\n",
            "Deleted orphan label: stain_93.txt\n",
            "Deleted orphan label: stain_57.txt\n",
            "Deleted orphan label: stain_347.txt\n",
            "Deleted orphan label: stain_92.txt\n",
            "Deleted orphan label: stain_71.txt\n",
            "Deleted orphan label: stain_334.txt\n",
            "Deleted orphan label: stain_360.txt\n",
            "Deleted orphan label: stain_51.txt\n",
            "Deleted orphan label: stain_90.txt\n",
            "Deleted orphan label: stain_370.txt\n",
            "Deleted orphan label: stain_382.txt\n",
            "Deleted orphan label: stain_372.txt\n",
            "Deleted orphan label: stain_393.txt\n",
            "Deleted orphan label: stain_47.txt\n",
            "Deleted orphan label: stain_77.txt\n",
            "Deleted orphan label: stain_397.txt\n",
            "Deleted orphan label: stain_352.txt\n",
            "Deleted orphan label: stain_367.txt\n",
            "Deleted orphan label: stain_73.txt\n",
            "Deleted orphan label: stain_5.txt\n",
            "Deleted orphan label: stain_343.txt\n",
            "Deleted orphan label: stain_361.txt\n",
            "Deleted orphan label: stain_68.txt\n",
            "Deleted orphan label: stain_390.txt\n",
            "Deleted orphan label: stain_70.txt\n",
            "Deleted orphan label: stain_64.txt\n",
            "Deleted orphan label: stain_46.txt\n",
            "Deleted orphan label: stain_378.txt\n",
            "Deleted orphan label: stain_67.txt\n",
            "Deleted orphan label: stain_392.txt\n",
            "Deleted orphan label: stain_48.txt\n",
            "Deleted orphan label: stain_368.txt\n",
            "Deleted orphan label: stain_4.txt\n",
            "Deleted orphan label: stain_59.txt\n",
            "Deleted orphan label: stain_358.txt\n",
            "Deleted orphan label: stain_375.txt\n",
            "Deleted orphan label: stain_340.txt\n",
            "Deleted orphan label: stain_381.txt\n",
            "Deleted orphan label: stain_398.txt\n",
            "Deleted orphan label: stain_9.txt\n",
            "Deleted orphan label: stain_61.txt\n",
            "Deleted orphan label: stain_386.txt\n",
            "Deleted orphan label: stain_342.txt\n",
            "Deleted orphan label: stain_91.txt\n",
            "Deleted orphan label: stain_374.txt\n",
            "Deleted orphan label: stain_349.txt\n",
            "Deleted orphan label: stain_38.txt\n",
            "Deleted orphan label: stain_49.txt\n",
            "Deleted orphan label: stain_387.txt\n",
            "Deleted orphan label: stain_379.txt\n",
            "Deleted orphan label: stain_75.txt\n",
            "Deleted orphan label: stain_62.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set paths\n",
        "labels_dir = '/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/train'  # or val/test\n",
        "images_dir = '/content/drive/MyDrive/Colab_Notebooks/Dataset/images/train'  # or val/test\n",
        "\n",
        "# Get list of image filenames without extensions\n",
        "image_basenames = {os.path.splitext(f)[0] for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))}\n",
        "\n",
        "# Loop through label files\n",
        "for label_file in os.listdir(labels_dir):\n",
        "    label_basename = os.path.splitext(label_file)[0]\n",
        "    if label_basename not in image_basenames:\n",
        "        # Orphan label found — delete it\n",
        "        os.remove(os.path.join(labels_dir, label_file))\n",
        "        print(f\"Deleted orphan label: {label_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy6VKHBwrlD6",
        "outputId": "a2837c23-5a8f-4e6a-8ec6-c4ec246a5f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted orphan label: stain_394.txt\n",
            "Deleted orphan label: stain_141.txt\n",
            "Deleted orphan label: stain_323.txt\n",
            "Deleted orphan label: stain_146.txt\n",
            "Deleted orphan label: stain_54.txt\n",
            "Deleted orphan label: stain_55.txt\n",
            "Deleted orphan label: stain_207.txt\n",
            "Deleted orphan label: stain_240.txt\n",
            "Deleted orphan label: stain_122.txt\n",
            "Deleted orphan label: stain_251.txt\n",
            "Deleted orphan label: stain_134.txt\n",
            "Deleted orphan label: stain_377.txt\n",
            "Deleted orphan label: stain_371.txt\n",
            "Deleted orphan label: stain_245.txt\n",
            "Deleted orphan label: stain_40.txt\n",
            "Deleted orphan label: stain_250.txt\n",
            "Deleted orphan label: stain_144.txt\n",
            "Deleted orphan label: stain_128.txt\n",
            "Deleted orphan label: stain_396.txt\n",
            "Deleted orphan label: stain_41.txt\n",
            "Deleted orphan label: stain_353.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set paths\n",
        "labels_dir = '/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/test'  # or val/test\n",
        "images_dir = '/content/drive/MyDrive/Colab_Notebooks/Dataset/images/test'  # or val/test\n",
        "\n",
        "# Get list of image filenames without extensions\n",
        "image_basenames = {os.path.splitext(f)[0] for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))}\n",
        "\n",
        "# Loop through label files\n",
        "for label_file in os.listdir(labels_dir):\n",
        "    label_basename = os.path.splitext(label_file)[0]\n",
        "    if label_basename not in image_basenames:\n",
        "        # Orphan label found — delete it\n",
        "        os.remove(os.path.join(labels_dir, label_file))\n",
        "        print(f\"Deleted orphan label: {label_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktOgrDjqrnU-",
        "outputId": "218504c6-6a70-4088-a853-da4615138c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted orphan label: stain_113.txt\n",
            "Deleted orphan label: stain_129.txt\n",
            "Deleted orphan label: stain_125.txt\n",
            "Deleted orphan label: stain_127.txt\n",
            "Deleted orphan label: stain_110.txt\n",
            "Deleted orphan label: stain_136.txt\n",
            "Deleted orphan label: stain_100.txt\n",
            "Deleted orphan label: stain_308.txt\n",
            "Deleted orphan label: stain_261.txt\n",
            "Deleted orphan label: stain_356.txt\n",
            "Deleted orphan label: stain_366.txt\n",
            "Deleted orphan label: stain_348.txt\n",
            "Deleted orphan label: stain_137.txt\n",
            "Deleted orphan label: stain_264.txt\n",
            "Deleted orphan label: stain_316.txt\n",
            "Deleted orphan label: stain_76.txt\n",
            "Deleted orphan label: stain_325.txt\n",
            "Deleted orphan label: stain_383.txt\n",
            "Deleted orphan label: stain_274.txt\n",
            "Deleted orphan label: stain_50.txt\n",
            "Deleted orphan label: stain_247.txt\n",
            "Deleted orphan label: stain_23.txt\n",
            "Deleted orphan label: stain_239.txt\n",
            "Deleted orphan label: stain_307.txt\n",
            "Deleted orphan label: stain_363.txt\n",
            "Deleted orphan label: stain_153.txt\n",
            "Deleted orphan label: stain_270.txt\n",
            "Deleted orphan label: stain_350.txt\n",
            "Deleted orphan label: stain_263.txt\n",
            "Deleted orphan label: stain_162.txt\n",
            "Deleted orphan label: stain_25.txt\n",
            "Deleted orphan label: stain_58.txt\n",
            "Deleted orphan label: stain_262.txt\n",
            "Deleted orphan label: stain_69.txt\n",
            "Deleted orphan label: stain_299.txt\n",
            "Deleted orphan label: stain_60.txt\n",
            "Deleted orphan label: stain_345.txt\n",
            "Deleted orphan label: stain_231.txt\n",
            "Deleted orphan label: stain_357.txt\n",
            "Deleted orphan label: stain_255.txt\n",
            "Deleted orphan label: stain_318.txt\n",
            "Deleted orphan label: stain_36.txt\n",
            "Deleted orphan label: stain_283.txt\n",
            "Deleted orphan label: stain_45.txt\n",
            "Deleted orphan label: stain_385.txt\n",
            "Deleted orphan label: stain_380.txt\n",
            "Deleted orphan label: stain_281.txt\n",
            "Deleted orphan label: stain_43.txt\n",
            "Deleted orphan label: stain_30.txt\n",
            "Deleted orphan label: stain_233.txt\n",
            "Deleted orphan label: stain_211.txt\n",
            "Deleted orphan label: stain_56.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Set paths\n",
        "labels_dir = '/content/drive/MyDrive/Colab_Notebooks/Dataset/labels/val'  # or val/test\n",
        "images_dir = '/content/drive/MyDrive/Colab_Notebooks/Dataset/images/val'  # or val/test\n",
        "\n",
        "# Get list of image filenames without extensions\n",
        "image_basenames = {os.path.splitext(f)[0] for f in os.listdir(images_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))}\n",
        "\n",
        "# Loop through label files\n",
        "for label_file in os.listdir(labels_dir):\n",
        "    label_basename = os.path.splitext(label_file)[0]\n",
        "    if label_basename not in image_basenames:\n",
        "        # Orphan label found — delete it\n",
        "        os.remove(os.path.join(labels_dir, label_file))\n",
        "        print(f\"Deleted orphan label: {label_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5N7-5H0t4r2"
      },
      "source": [
        "**5) Training Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rvnOE8NdC_F",
        "outputId": "689d5c7b-5d9c-4a2c-c994-79db92dee0ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17511, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 17511 (delta 5), reused 0 (delta 0), pack-reused 17493 (from 3)\u001b[K\n",
            "Receiving objects: 100% (17511/17511), 16.62 MiB | 21.96 MiB/s, done.\n",
            "Resolving deltas: 100% (12000/12000), done.\n",
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: WARNING ⚠️ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n",
            "2025-07-13 13:19:31.033069: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752412771.366429    6522 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752412771.462697    6522 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=fabric_data.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=75, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=FabricDefectDetection, name=yolov5s_results, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['thop>=0.1.1'] not found, attempting AutoUpdate...\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 0.8s\n",
            "WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "YOLOv5 🚀 v7.0-422-g2540fd4c Python-3.11.13 torch-2.6.0+cu124 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir FabricDefectDetection', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100% 755k/755k [00:00<00:00, 14.1MB/s]\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt to yolov5m.pt...\n",
            "100% 40.8M/40.8M [00:00<00:00, 191MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=6\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
            "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
            "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
            "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
            "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
            "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
            "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
            "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
            "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
            "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
            " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
            " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
            " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
            " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
            " 24      [17, 20, 23]  1     44451  models.yolo.Detect                      [6, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
            "Model summary: 291 layers, 20891523 parameters, 20891523 gradients, 48.3 GFLOPs\n",
            "\n",
            "Transferred 475/481 items from yolov5m.pt\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005), 82 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0m1 validation error for InitSchema\n",
            "size\n",
            "  Field required [type=missing, input_value={'scale': (0.8, 1.0), 'ra...: None, 'strict': False}, input_type=dict]\n",
            "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/Colab_Notebooks/Dataset/labels/train.cache... 1044 images, 588 backgrounds, 0 corrupt: 100% 1044/1044 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/Colab_Notebooks/Dataset/labels/val.cache... 150 images, 23 backgrounds, 0 corrupt: 100% 150/150 [00:00<?, ?it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.12 anchors/target, 0.972 Best Possible Recall (BPR). Anchors are a poor fit to dataset ⚠️, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 762 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7531: 100% 1000/1000 [00:00<00:00, 2133.91it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9987 best possible recall, 3.46 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.261/0.753-mean/best, past_thr=0.489-mean: 33,30, 25,126, 72,66, 380,28, 191,155, 686,77, 231,372, 208,585, 565,303\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mDone ✅ (optional: update model *.yaml to use these anchors in the future)\n",
            "Plotting labels to FabricDefectDetection/yolov5s_results/labels.jpg... \n",
            "/content/yolov5/train.py:357: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mFabricDefectDetection/yolov5s_results\u001b[0m\n",
            "Starting training for 75 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "  0% 0/66 [00:00<?, ?it/s]/content/yolov5/train.py:414: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd /content/yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 75 --data fabric_data.yaml --weights yolov5m.pt --project FabricDefectDetection --name yolov5s_results --exist-ok\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Lfs for pushing the model file into the repository"
      ],
      "metadata": {
        "id": "p4bR8IUeJ4vm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1gXJ2kZpTx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbb19c2-ca46-4146-e745-f54fc3f28eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Git LFS initialized.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get install git-lfs -y\n",
        "!git lfs install\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone Your GitHub Repo"
      ],
      "metadata": {
        "id": "f2VPpNiqK76L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Kirtan-Mudaliyar/fabric-defect-detector.git\n",
        "%cd fabric-defect-detector\n"
      ],
      "metadata": {
        "id": "NVulTVc4hnhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173c6eb4-35ff-4b4b-856e-be2f02559548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fabric-defect-detector'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 186 (delta 16), reused 0 (delta 0), pack-reused 152 (from 1)\u001b[K\n",
            "Receiving objects: 100% (186/186), 5.61 MiB | 12.06 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n",
            "/content/fabric-defect-detector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Track .pt Files Using Git LFS"
      ],
      "metadata": {
        "id": "XSbBhTnoLKb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs track \"*.pt\"\n",
        "!git add .gitattributes\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIVqpQqxLAcl",
        "outputId": "790aaca9-527a-49d0-e26c-2e9212d4b353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracking \"*.pt\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the Weights to Repo Directory Pushing into repository"
      ],
      "metadata": {
        "id": "7Yp3nociLYer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "lzEs27zYLNVH",
        "outputId": "ed72eab0-5069-4afc-c0b9-a54ab4c07ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-22d4c862-57f3-4b90-8073-0fa76725d909\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-22d4c862-57f3-4b90-8073-0fa76725d909\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving best.pt to best.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "57b1OhMraWXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Double-check both file and folder exist\n",
        "if os.path.exists(\"best.pt\") and os.path.exists(\"fabric-defect-detector\"):\n",
        "    shutil.move(\"best.pt\", \"fabric-defect-detector/best.pt\")\n",
        "    print(\"Model moved successfully!\")\n",
        "else:\n",
        "    print(\"Check if best.pt is uploaded and repo folder exists.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyxiB4vRNJ5v",
        "outputId": "f8789d7d-ce61-4278-b465-a6bea1c90b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if best.pt is uploaded and repo folder exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_path = \"weights/best.pt\"\n",
        "\n",
        "\n",
        "# Check existence\n",
        "if os.path.exists(model_path):\n",
        "    size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
        "    print(f\"✅ Model found at: {model_path}\")\n",
        "    print(f\"📦 File size: {size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(\"❌ best.pt not found in the expected path.\")\n"
      ],
      "metadata": {
        "id": "SMsyP-fbNswC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c851c58-6931-4ae9-d64d-5f02af96da3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model found at: weights/best.pt\n",
            "📦 File size: 40.82 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Go to project\n",
        "%cd /content/fabric-defect-detector\n",
        "\n",
        "# 2. Reset Git and clean up\n",
        "!rm -rf .git\n",
        "!git init\n",
        "!git lfs install\n",
        "!git config user.email \"kirtanmudaliyar@gmail.com\"\n",
        "!git config user.name \"Kirtan-Mudaliyar\"\n",
        "\n",
        "# 3. Add GitHub remote (overwrite if already exists)\n",
        "from getpass import getpass\n",
        "token = getpass(\"Enter your GitHub token:\")\n",
        "!git remote add origin https://Kirtan-Mudaliyar:{token}@github.com/Kirtan-Mudaliyar/fabric-defect-detector.git\n",
        "\n",
        "# 4. Rename local branch to match GitHub\n",
        "!git branch -M main\n",
        "\n",
        "# 5. Pull remote with merge (to avoid divergence error)\n",
        "!git pull origin main --allow-unrelated-histories --no-rebase\n",
        "\n",
        "# 6. Create weights directory and move the model file there\n",
        "!mkdir -p weights\n",
        "!cp /content/best.pt weights/\n",
        "\n",
        "# 7. Add and commit only the weights file\n",
        "!git add weights/best.pt\n",
        "!git commit -m \"Add YOLOv5m model via Git LFS\"\n",
        "\n",
        "# 8. Push to GitHub\n",
        "!git push origin main\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5NzFCXVO4Ga",
        "outputId": "91c04d2e-d402-423d-feae-3b690aa17487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fabric-defect-detector\n",
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/fabric-defect-detector/.git/\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Enter your GitHub token:··········\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 186 (delta 16), reused 0 (delta 0), pack-reused 152 (from 1)\u001b[K\n",
            "Receiving objects: 100% (186/186), 5.61 MiB | 12.27 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n",
            "From https://github.com/Kirtan-Mudaliyar/fabric-defect-detector\n",
            " * branch            main       -> FETCH_HEAD\n",
            " * [new branch]      main       -> origin/main\n",
            "cp: cannot stat '/content/best.pt': No such file or directory\n",
            "[main d07df17] Add YOLOv5m model via Git LFS\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 weights/best.pt\n",
            "Enumerating objects: 6, done.\n",
            "Counting objects: 100% (6/6), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 37.65 MiB | 6.60 MiB/s, done.\n",
            "Total 4 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/Kirtan-Mudaliyar/fabric-defect-detector.git\n",
            "   9e0b9ea..d07df17  main -> main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whBX3LIDYs0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}